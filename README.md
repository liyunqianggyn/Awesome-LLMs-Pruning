# Awesome LLMs Pruning 
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Integrating useful resources into one repository for large models pruning papers, including one sentence take-away summary, explanation notes such as paper's challenges,  blogs or videos, paper tags, source code links and venue.

Please feel free to [pull requests](https://github.com/liyunqianggyn/Awesome-LLMs-Pruning/pulls) or [open an issue](https://github.com/liyunqianggyn/Awesome-LLMs-Pruning/issues) to add papers.

:fire: Keep updating... 
Please star it if you find it helpful:)

## Table of Contents

- [Tags of Pruning](#tags-of-pruning)

- [2024 Venues](#2024)

- [2023 Venues](#2023)


### Tags of Pruning

[//]: # (We refer to our recent accepted ICCV paper [_differentiable transportation pruning_]&#40;https://arxiv.org/abs/2307.08483&#41;   for summarizing these tags.)
Click on the badge, such as [![Budget](https://img.shields.io/badge/Data-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md), will direct you to the corresponding explanation file. 


| [![Type](https://img.shields.io/badge/Type-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md) | [![Criteria](https://img.shields.io/badge/Criteria-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) | [![Budget](https://img.shields.io/badge/Budget-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) | [![Budget](https://img.shields.io/badge/Data-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md) | [![Type](https://img.shields.io/badge/Retrain-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md) | [![Type](https://img.shields.io/badge/Weight-yellow)](concepts/weight_update.md) |
|:-------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:|
|                                                                        `Unstructured`                                                                         |                                                                     `Magnitude`                                                                      |                                                        `Sparsity e.g. layer or global`                                                        |                                                                `Data-free`                                                                |                                                                    `Without`                                                                    |                                     `Frozen`                                     |
|                                                                         `Structured`                                                                          |                                              `Taylor e.g.` [`Hessian`](https://arxiv.org/abs/1906.10771)                                              |                                                                    `FLOPs`                                                                    |                                                               `Calibration`                                                               |                                               `Efficient e.g.` [`LoRA`](concepts/details/LoRA.md)                                               |                                     `Update`                                     |
|                                                                       `Semi-structured`                                                                       |                                                                       `Fisher`                                                                       |                                                                   `Latency`                                                                   |                                                                  `Small`                                                                  |                                                                   `Extensive`                                                                   |                                       `-`                                        |
|                                                                            `Other`                                                                            |                                                                     `Trainable`                                                                      |                                                                   `Energy`                                                                    |                                                                 `Medium`                                                                  |                                                                    `Scratch`                                                                    |                                       `-`                                        |
|                                                                             ` - `                                                                             |                                                                       `Other`                                                                        |                                                                    `Other`                                                                    |                                                                  `Large`                                                                  |                                                                     `Other`                                                                     |                                       `-`                                        |
### 2025

| <div style="width: 150px">Title & Take-away </div>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       <div style="width: 200px"> <span style="display: inline; opacity: 0;">..................</span> Tags <span style="display: inline; opacity: 0;">..................</span>   </div>                                                                                                                                                                                                                                                                                                                                                                                                        |                         <div style="width: 50px">Note </div>                          |             <div style="width: 50px">Code </div>             |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------:|:------------------------------------------------------------:|
| [![Star](https://img.shields.io/github/stars/arcee-ai/PruneMe.svg?style=social&label=Star)](https://github.com/arcee-ai/PruneMe)  [![Publish](https://img.shields.io/badge/Submission-ICLR'25-blue)]() <br> [The Unreasonable Ineffectiveness of the Deeper Layers](https://arxiv.org/abs/2403.17887) <br>   A simple layer/depth-pruning to remove n consecutive or contiguous layers from popular families of open-weight pretrained LLMs by minimizing the angular distance between layers' representations.  Parameter-efficient finetuning method is applied to further reduce computational resources of finetuning. | [![Type](https://img.shields.io/badge/Type-Structured--Depth-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Angular_Distance-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Layer_Numbers-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Efficient_LoRA-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md) <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md) | [Challenge](challenge/challenge.md/#prune-unreasonable-ineffectivenes-deeplayer-2025) |        [PyTorch](https://github.com/arcee-ai/PruneMe)        |

[//]: # (| [![Star]&#40;https://img.shields.io/github/stars/Nota-NetsPresso/shortened-llm.svg?style=social&label=Star&#41;]&#40;https://github.com/Nota-NetsPresso/shortened-llm&#41;  [![Publish]&#40;https://img.shields.io/badge/Submission-ICLR'24W-blue&#41;]&#40;&#41; <br> [Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods]&#40;https://arxiv.org/abs/2402.02834&#41;  <br>   A                                                                                                                                                                                                                                        | [![Type]&#40;https://img.shields.io/badge/Type-Structured--Depth-blue&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md&#41;  <br> [![Type]&#40;https://img.shields.io/badge/Criteria-Angular_Distance-C2A4A6&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md&#41; <br> [![Type]&#40;https://img.shields.io/badge/Budget-Layer_Numbers-brown&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md&#41; <br> [![Type]&#40;https://img.shields.io/badge/Data-Calibration-green&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md&#41;   <br> [![Type]&#40;https://img.shields.io/badge/Retrain-Efficient_LoRA-orange&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md&#41; <br> [![Type]&#40;https://img.shields.io/badge/Weight-Update-orange&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md&#41; | [Challenge]&#40;challenge/challenge.md/#prune-unreasonable-ineffectivenes-deeplayer-2025&#41; | [PyTorch]&#40;https://github.com/Nota-NetsPresso/shortened-llm&#41;  |)


### 2024

| <div style="width: 150px">Title & Take-away </div>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <dir style="width: 100px"> Tags    </dir>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                             <div style="width: 50px">Note </div>                                                                                                                                             |                              <div style="width: 50px">Code </div>                               |
|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------:|
| [![Star](https://img.shields.io/github/stars/NVlabs/Minitron.svg?style=social&label=Star)](https://github.com/NVlabs/Minitron)  [![Publish](https://img.shields.io/badge/Arxiv'24-blue)]() <br> [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2407.14679) <br>  Prune LLMs structurally along different axes such as layer, neuron, head, and embedding channel, similar to NAS that searches over different dimensions. Difference lies in the defined search space that for pruning a *pre-trained* large model as search space (simpler) while NAS searches over a manually-pre-defined search space (more complex)  from scratch.   Different proxy importance scores are estimated per axis for pruning separately. Retraining with knowledge distillation requires up to 40x fewer training tokens.                       |                        [![Type](https://img.shields.io/badge/Type-Structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Activation/Perplexity...-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Parameter-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration+Medium_1.8B-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Extensive-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md) <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                        |                                                               [Challenge](challenge/challenge.md/#prune-distill-llama-31-2024)  <br/>[Blog](https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/)                                                               |                         [PyTorch](https://github.com/NVlabs/Minitron)                           |
| [![Star](https://img.shields.io/github/stars/d-matrix-ai/keyformer-llm.svg?style=social&label=Star)](https://github.com/d-matrix-ai/keyformer-llm) [![Publish](https://img.shields.io/badge/Conference-MLSys'24-blue)]() <br> [Keyformer: KV Cache reduction through attention sparsification for Efficient Generative Inference](https://arxiv.org/abs/2403.09054) <br>  Keyformer, a successor to [H2O](https://arxiv.org/abs/2306.14048) (see below),  uses a *Gumbel softmax-based score function* instead of solely attention scores in H2O, for dynamically identifying and retaining top-k key tokens, to reduce [*KV cache*](concepts/details/KV_cache.md) size. A sliding window drawn from [Sparse Transformer](https://arxiv.org/abs/1904.10509) is used to retain (not prune) w recent *representative* tokens, yileding a mixture of recent and key tokens. |                                       [![Type](https://img.shields.io/badge/Type-Other-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Other-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--KV_cache-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Data_free-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md) <br> [![Type](https://img.shields.io/badge/Weight-Frozen-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                                        | [Challenge](challenge/challenge.md/#keyformer-attention-sparsification-2024) <br/>[Blog](https://medium.com/@d-matrix/keyformer-kv-cache-reduction-through-attention-sparsification-for-efficient-generative-inference-f4b659f5594e)   <br/>[Summary](https://github.com/d-matrix-ai/keyformer-llm/blob/main/blog/README.md) |                     [PyTorch](https://github.com/d-matrix-ai/keyformer-llm)                     |
| [![Star](https://img.shields.io/github/stars/locuslab/wanda.svg?style=social&label=Star)](https://github.com/locuslab/wanda) [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]() <br> [A Simple and Effective Pruning Approach for Large Language Models](https://arxiv.org/abs/2306.11695) <br> A pruning metric termed Wanda that considers both weight magnitudes and input activation norms to prune weights *per-output basis* instead of layer-wise, requiring no retraining or weight update.  A simplified version of [*SparseGPT*](https://arxiv.org/abs/2301.00774).                                                                                                                                                                                                                                                                          |                                 [![Type](https://img.shields.io/badge/Type-Un/Semi--structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Wanda-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--layer-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md) <br> [![Type](https://img.shields.io/badge/Weight-Frozen-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                                 |                                [Challenge](challenge/challenge.md/#wanda-simple-effective-pruning-approach-2024) <br/>[Blog](https://www.linkedin.com/pulse/efficient-model-pruning-large-language-models-wandas-ayoub-kirouane/) <br> [Reviews](https://openreview.net/forum?id=PxoFut3dWW)                                 |                          [PyTorch](https://github.com/locuslab/wanda)                           |
| [![Publish](https://img.shields.io/badge/Arxiv'24-blue)]() <br> [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) <br> 1.58 bits to quantize every single parameter of the LLM in ternary -1, 0, or +1. This can be viewed as an 1-bit binarization -1 or 1 along with unstructured pruning 0. It matches the full-precision Transformer LLM with the same model size and training tokens  when trained from scratch, with 1.58-bit weights and 8-bit activations.                                                                                                                                                                                                                                                                                                                                                   |                                         [![Type](https://img.shields.io/badge/Type-Unstructured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Trainable-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Large-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Scratch-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md) <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                                         |                                                                                                [Challenge](challenge/challenge.md/#1-bit-llms-approach-2024) <br/>[Discussion](https://huggingface.co/papers/2402.17764) <br>                                                                                                |                [PyTorch](https://github.com/microsoft/unilm/tree/master/bitnet)                 |
| [![Star](https://img.shields.io/github/stars/LinkAnonymous/BESA.svg?style=social&label=Star)](https://github.com/LinkAnonymous/BESA) [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]() <br> [BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation](https://openreview.net/forum?id=gC6JTEU3jl)  <br> Adaptively allocate optimal sparsity ratio of each layer within a *transformer block* by minizming block-wise reconstruction error. To do so,  a parameter-efficient algorithm is developed with ony optimizing few learnable coefficients _e.g.,_ 100.  Pre-trained weights are frozen.                                                                                                                                                                                                                   |                    [![Type](https://img.shields.io/badge/Type-Unstructured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Wanda-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--block-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)                                  <br> [![Type](https://img.shields.io/badge/Weight-Frozen-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                    |                                               [Challenge](challenge/challenge.md/#besa-blockwise-parameter-efficient-sparsity-allocation-2024) <br/>                                                                    [Reviews](https://openreview.net/forum?id=gC6JTEU3jl)                                                |                        [PyTorch](https://github.com/LinkAnonymous/BESA)                         |
| [![Star](https://img.shields.io/github/stars/princeton-nlp/LLM-Shearing.svg?style=social&label=Star)](https://github.com/princeton-nlp/LLM-Shearing) [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]()  <br>[Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning](https://arxiv.org/abs/2310.06694) <br>  In first stage, training-aware pruning learns masks  satisfying specified target by imposing regularization on ~0.4B tokens; then retrain on other ~5B tokens of _RedPajama_ dataset.   Dynamic batch loading method to update the composition of sampled data per mini-batch across different domains.                                                                                                                                                                                                          |                   [![Type](https://img.shields.io/badge/Type-Structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Trainable-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--mix-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Medium~50B-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Extensive-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)                                  <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                    |                                                               [Challenge](challenge/challenge.md/#sheared-llama-2024) <br/>                  [Blog](https://xiamengzhou.github.io/sheared-llama/) <br/> [Reviews](https://openreview.net/forum?id=09iOdaeOzp)                                                                |                    [PyTorch](https://github.com/princeton-nlp/LLM-Shearing)                     |
| [![Star](https://img.shields.io/github/stars/luuyin/OWL.svg?style=social&label=Star)](https://github.com/luuyin/OWL) [![Publish](https://img.shields.io/badge/Submission-ICLR'24-blue)]()  <br>[Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity](https://arxiv.org/abs/2310.05175)  <br>  Allocate non-uniform sparsity ratios across different layers guided by the principle that  a layer with higher proportion of [_outliers_](concepts/other_concepts.md/#Outliers-in-LLMs) should have a lower sparsity, then apply the more tailored layer-wise sparsity directly into Wanda and SparseGPT.                                                                                                                                                                                                                   |                [![Type](https://img.shields.io/badge/Type-Unstructured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Wanda/Hessian-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--layer-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)                                  <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                |                                                                           [Challenge](challenge/challenge.md/#outlier-weighed-layerwise-sparsity-owl-2024) <br/>                             [Reviews](https://openreview.net/forum?id=pOBvr1PxFd)                                                                           |                            [PyTorch](https://github.com/luuyin/OWL)                             |
| [![Star](https://img.shields.io/github/stars/biomedical-cybernetics/Relative-importance-and-activation-pruning.svg?style=social&label=Star)](https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning) [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]()  <br>[Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models](https://openreview.net/forum?id=Tr0lPx9woF)  <br>  Propose a new pruning criteria named [_RIA_](concepts/criteria.md) for LLMs. In [N:M](concepts/types.md/#semi-structured) structures, introduce a column permutation matrix for score matrix to maximize the total retained weight importance. No retraining.                                                                                                                                                       |                     [![Type](https://img.shields.io/badge/Type-Un/Semi--structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-RIA-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)                                  <br> [![Type](https://img.shields.io/badge/Weight-Frozen-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                     |                                                                                               [Challenge](challenge/challenge.md/#plug-and-play-2024)<br/>               [Reviews](https://openreview.net/forum?id=Tr0lPx9woF)                                                                                               | [PyTorch](https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning) |
| [![Publish](https://img.shields.io/badge/Submission-ICLR'24-blue)]()  <br>[Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models](https://arxiv.org/abs/2310.05015)  <br>  Retrain LLMs' weights with lightweight  [*LoRA*](concepts/details/LoRA.md), and optimize _structured-pruning masks_ with efficient trainable parameters in differentiable way on  instruction-tuning [Alpaca](https://github.com/gururise/AlpacaDataCleaned) dataset. Collaborative prompt is used to help pruning task.                                                                                                                                                                                                                                                                                                                            |               [![Type](https://img.shields.io/badge/Type-Structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Trainable-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--global-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Small--50K-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Efficient--LoRA-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)                                  <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)               |                                                                                         [Challenge](challenge/challenge.md/#compresso-2024) <br/>                             [Reviews](https://openreview.net/forum?id=ktiikNTgK5)                                                                                          |               [PyTorch](https://github.com/microsoft/Moonlit/tree/main/Compresso)               |
| [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]()  <br>[Scaling Laws for Sparsely-Connected Foundation Models](https://arxiv.org/abs/2309.08520)  <br>   Discover [_scaling law_](concepts/other_concepts.md/#scaling-laws) of weight sparsity, formulating the scaling relationships between weight sparsity, non-zero parameter numbers, and training data size. Revealing an increasing _optimal sparsity_ with more training data and offering insights for improved computational efficiency.                                                                                                                                                                                                                                                                                                                                                    |                [![Type](https://img.shields.io/badge/Type-Un/Semi--structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Trainable-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Large--150B+-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Extensive-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)                                  <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                 |                                                                                    [Challenge](challenge/challenge.md/#scaling-laws-sparse-2024) <br/>                             [Reviews](https://openreview.net/forum?id=i9K2ZWkYIP)                                                                                     |                                                -                                                |
| [![Star](https://img.shields.io/github/stars/Qualcomm-AI-research/llm-surgeon.svg?style=social&label=Star)](https://github.com/Qualcomm-AI-research/llm-surgeon) [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]()  <br>[The LLM Surgeon](https://arxiv.org/abs/2312.17244)  <br>    This paper introduces LLM Surgeon, a method that enhances the efficiency of second-order Hessian-based pruning techniques, such as Optimal Brain Surgeon, by employing Kronecker-factored approximations of the Fisher information matrix. The approach establishes closed-form solutions. Prune OPT models and Llamav2-7B by 20%-30% achieves a negligible loss in performance.                                                                                                                                                                                 |       [![Type](https://img.shields.io/badge/Type-Un/Semi--/Structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Hessian--Approximation-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--Global-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)                                  <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)       |                                                                                    [Challenge](challenge/challenge.md/#the-llm-surgeon-iclr-2024) <br/>                             [Reviews](https://openreview.net/forum?id=DYIIRgwg2i)                                                                                    |                 [PyTorch](https://github.com/Qualcomm-AI-research/llm-surgeon)                  |
| [![Star](https://img.shields.io/github/stars/snudm-starlab/k-prune.svg?style=social&label=Star)](https://github.com/snudm-starlab/k-prune)  [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]() <br> [Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models](https://openreview.net/forum?id=s2NjWfaYdZ)  <br>      Retraining-free pruning for  encoder-based language model such as [*BERT*](https://arxiv.org/abs/1810.04805)   to preserve the knowledge of PLMs through sublayer-wise iterative pruning, from the bottom to the top sublayer.                                                                                                                                                                                                                                                                              |                          [![Type](https://img.shields.io/badge/Type-Head/Neuron-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-KL-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-FLOPs-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Efficient-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)                                  <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                          |                                                                                            [Challenge](challenge/challenge.md/#kprune-iclr-2024) <br/>                     [Reviews](https://openreview.net/forum?id=s2NjWfaYdZ)                                                                                             |                       [PyTorch](https://github.com/snudm-starlab/k-prune)                       |
| [![Star](https://img.shields.io/github/stars/zyxxmu/DSnoT.svg?style=social&label=Star)](https://github.com/zyxxmu/DSnoT) [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]() <br>[Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs](https://arxiv.org/abs/2310.08915) <br>  Dynamic Sparse No Training (DSNT) involves iterative pruning-and-growing steps that only updating sparse mask yet mask adaptation by minimizing reconstruction error e.g. proxy of perplexity;  Enable a higher 60% or 70% sparsity rate; Training-free.                                                                                                                                                                                                                                                                                               | &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp;     <br>         [![Type](https://img.shields.io/badge/Type-Unstructured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br>  [![Type](https://img.shields.io/badge/Criteria-Magnitude/Hessian/Wanda-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)       <br> [![Type](https://img.shields.io/badge/Weight-Frozen-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md) |                                                                                               [Challenge](challenge/challenge.md/#dsot-iclr-2024) <br/>                  [Reviews](https://openreview.net/forum?id=1ndDmZdT4g)                                                                                               |                           [PyTorch](https://github.com/zyxxmu/DSnoT)                            |
| [![Star](https://img.shields.io/github/stars/AlibabaResearch/flash-llm.svg?style=social&label=Star)](https://github.com/AlibabaResearch/flash-llm) [![Publish](https://img.shields.io/badge/Conference-VLDB'24-blue)]() <br>[Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity](https://arxiv.org/abs/2309.10285) <br>  An effective software framework for tensor cores (do not allow skipping arbitrary element-level computations) based unstructured SpMM, leveraging on-chip resources for efficient sparse data extraction and computation/memory-access overlapping. Improving memory bandwidth utilization in GPU.                                                                                                                                                                             |                                   [![Type](https://img.shields.io/badge/Type-Unstructured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br>  [![Type](https://img.shields.io/badge/Criteria-Magnitude-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--layer-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Other-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Other-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)       <br> [![Type](https://img.shields.io/badge/Weight-Other-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                                    |                                                                                                                                                              -                                                                                                                                                               |                   [Python/C++](https://github.com/AlibabaResearch/flash-llm)                    |
| [![Star](https://img.shields.io/github/stars/Nota-NetsPresso/shortened-llm.svg?style=social&label=Star)](https://github.com/Nota-NetsPresso/shortened-llm) [![Publish](https://img.shields.io/badge/Conference-ICLRW'24-blue)]() <br>[Shortened LLaMA: A Simple Depth Pruning for Large Language Models](https://arxiv.org/abs/2402.02834) <br>  First identify unimportant Transformer blocks (bigger and coarse units), then perform one-shot pruning with Perplexity (PPL) as pruning criteria  and light [LoRA](concepts/details/LoRA.md) retraining.  Show fast inference and good zero-shot capabilities.                                                                                                                                                                                                                                                          |                         [![Type](https://img.shields.io/badge/Type-Structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br>  [![Type](https://img.shields.io/badge/Criteria-Perplexity-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration/Samll--50K-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Efficient--LoRA-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)       <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)                          |                                                                                                                                [Challenge](challenge/challenge.md/#shortenedllms-iclrw-2024)                                                                                                                                 |                   [PyTorch](https://github.com/Nota-NetsPresso/shortened-llm)                   |

[//]: # (| [![Publish]&#40;https://img.shields.io/badge/Submission-ICLR'24-blue&#41;]&#40;&#41;  <br>[LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning]&#40;https://arxiv.org/abs/2305.18403&#41;  <br>   lightweight  [*LoRA*]&#40;concepts/details/LoRA.md&#41;, and optimize _structured-pruning masks_                                                                                      | [![Type]&#40;https://img.shields.io/badge/Type-Structured-blue&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md&#41;  <br> [![Type]&#40;https://img.shields.io/badge/Criteria-Trainable-C2A4A6&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md&#41; <br> [![Type]&#40;https://img.shields.io/badge/Budget-Sparsity--global-brown&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md&#41; <br> [![Type]&#40;https://img.shields.io/badge/Data-Small--50K-green&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md&#41;   <br> [![Type]&#40;https://img.shields.io/badge/Retrain-Efficient--LoRA-orange&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md&#41;                                  <br> [![Type]&#40;https://img.shields.io/badge/Weight-Update-orange&#41;]&#40;https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md&#41; |                                                          [Challenge]&#40;challenge/challenge.md/#compresso-2024&#41; <br/>                             [Reviews]&#40;https://openreview.net/forum?id=ktiikNTgK5&#41;                                                          | [PyTorch]&#40;https://github.com/microsoft/Moonlit/tree/main/Compresso&#41;       |)

[//]: # (ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models)
[//]: # (SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression)

### 2023

| <div style="width: 150px">Title & Take-away </div>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <dir style="width: 100px"> Tags    </dir>                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                         <div style="width: 50px">Note </div>                                                         |            <div style="width: 50px">Code </div>             |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------:|
| [![Star](https://img.shields.io/github/stars/FMInference/H2O.svg?style=social&label=Star)](https://github.com/FMInference/H2O) [![Publish](https://img.shields.io/badge/Conference-NeurIPS'23-blue)]() <br> [H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/abs/2306.14048) <br> During prompt and generation phase, dynamically prune the unimportant tokens based on *accumulated* attention scores, yet maintaining a constant small Key-Value Cache ([*KV cache*](concepts/details/KV_cache.md) ) size with k tokens.                                                                                                                                     |               [![Type](https://img.shields.io/badge/Type-Other-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br> [![Type](https://img.shields.io/badge/Criteria-Other-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--KV_cache-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Data_free-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md) <br> [![Type](https://img.shields.io/badge/Weight-Frozen-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)               |                                                  [Challenge](challenge/challenge.md/#h2o-nips-2023)                                                  |        [PyTorch](https://github.com/FMInference/H2O)        |
| [![Star](https://img.shields.io/github/stars/IST-DASLab/sparsegpt.svg?style=social&label=Star)](https://github.com/IST-DASLab/sparsegpt) [![Publish](https://img.shields.io/badge/Conference-ICML'23-blue)]() <br> [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](https://arxiv.org/abs/2301.00774) <br> Post-training method for pruning LLMs in one-shot without any retraining. Updating weights by solving a *layer-wise weight reconstruction* problem.                                                                                                                                                                                                                                |       [![Type](https://img.shields.io/badge/Type-Un/Semi--structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)   <br> [![Type](https://img.shields.io/badge/Criteria-Hessian-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--layer-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Calibration-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md) <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)       | [Challenge](challenge/challenge.md/#sparsegpt-icml-2023)<br/> [Blog](https://neuralmagic.com/blog/sparsegpt-remove-100-billion-parameters-for-free/) |     [PyTorch](https://github.com/IST-DASLab/sparsegpt)      |
| [![Star](https://img.shields.io/github/stars/horseee/LLM-Pruner.svg?style=social&label=Star)](https://github.com/horseee/LLM-Pruner) [![Publish](https://img.shields.io/badge/Conference-NeurIPS'23-blue)]() <br> [LLM-Pruner: On the Structural Pruning of Large Language Models](https://arxiv.org/abs/2305.11627) <br>  First discover all coupled structures following [*Depgraph*](https://arxiv.org/abs/2301.12900), then estimate grouped importance of coupled structure on calibration, then prune less important groups, and last finetune with efficient [*LoRA*](concepts/details/LoRA.md) on [Alpaca](https://github.com/gururise/AlpacaDataCleaned) dataset consists of  50K instruction-response pairs. |     [![Type](https://img.shields.io/badge/Type-Structured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br>  [![Type](https://img.shields.io/badge/Criteria-Hessian-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity--layer-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Small--50K-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Efficient--LoRA-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)       <br> [![Type](https://img.shields.io/badge/Weight-Update-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md)     |                                              [Challenge](challenge/challenge.md/#llm-pruner-nips-2023)                                               |      [PyTorch](https://github.com/horseee/LLM-Pruner)       |
| [![Star](https://img.shields.io/github/stars/VITA-Group/essential_sparsity.svg?style=social&label=Star)](https://github.com/VITA-Group/essential_sparsity) [![Publish](https://img.shields.io/badge/Conference-NeurIPS'23-blue)]() <br>[The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter](https://arxiv.org/abs/2306.03805) <br>  Revisiting magnitude pruning and several interesting findings on pruning large scaled models. Most performances are reported with fine-tuned downstream tasks, except for that on modern-scale LLMs where _no retraining_ is performed.                                                                                                      | [![Type](https://img.shields.io/badge/Type-Unstructured-blue)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/types.md)  <br>  [![Type](https://img.shields.io/badge/Criteria-Magnitude-C2A4A6)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/criteria.md) <br> [![Type](https://img.shields.io/badge/Budget-Sparsity-brown)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/budget.md) <br> [![Type](https://img.shields.io/badge/Data-Small--tasks-green)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/data.md)   <br> [![Type](https://img.shields.io/badge/Retrain-Extensive/Without-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/finetune.md)       <br> [![Type](https://img.shields.io/badge/Weight-Update/Frozen-orange)](https://github.com/liyunqianggyn/LLMs-Pruning-All-In-One/blob/main/concepts/weight_update.md) |                                    [Challenge](challenge/challenge.md/#emergence-of-essential-sparsity-nips-2023)                                    | [PyTorch](https://github.com/VITA-Group/essential_sparsity) |

[//]: # (Deja vu: Contextual sparsity for efficient llms at inference time. )

[//]: # (### 2022)
[//]: # (Parameter-Efficient Sparsity for Large Language Models Fine-Tuning, IJCAI)


[//]: # (## Related Repotories)
[//]: # ([Awesome-Efficient-LLM]&#40;https://github.com/horseee/Awesome-Efficient-LLM&#41;)
[//]: # ([Awesome-LLM-Compression]&#40;https://github.com/HuangOwen/Awesome-LLM-Compression&#41;)


