### General Challenges in LLMs Pruning
- Extensive number of _parameters_ (billions to trillions) in foundation models poses _**computational**_ and
_**memory**_ challenges for training.
- To achieve general purpose, LLMs are pre-trained on large scale web-based datasets (trillions of tokens) that poses _**data**_ challenges for training.
- Fine-tuning LLMs on task-specific datasets may make the model _**overfit**_ to task-specific data, and _**collapse**_ to specific domain while lose ability for other domains.

